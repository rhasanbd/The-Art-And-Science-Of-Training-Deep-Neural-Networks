If github in unable to render a Jupyter notebook, copy the link of the notebook and enter into the nbviewer: https://nbviewer.jupyter.org/


Training Deep Artificial Neural Networks (DANNs) or Deep Neural Networks (DNNs) is **as much art as much science**. Following notebooks are designed to offer a deep understanding of the issues related to training DNNs.

- Notebook 1: Initialization of Weights - it is useful to solve the vanishing/exploding gradients problem in DNNs.

- Notebook 2: Exponentially Weighted Moving Average (EWMA) - it is a useful technique to reduce oscillations in Stochastic Gradient Descent. It is referred in the 3rd notebook.

- Notebook 3: Achillesâ€™ heel of Stochastic Gradient Descent - it provides a detail exploration of the limitation of the SGD based optimization and various faster optimization techniques. Unlike other notebooks, it's a PDF file. To access the references via hyperlinks it should be downloaded.
